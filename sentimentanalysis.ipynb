{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7763359,"sourceType":"datasetVersion","datasetId":4540583}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yonkotoshiro/sentimentanalysis?scriptVersionId=174467838\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Семантический анализ распознавания эмоций\nДанный датасет представляет собой большой набор текста для обучения распознавания эмоций. Всего имеется два столбца - текст и соответствующую ему эмоция.\nМы попробуем построить несколько несложных моделей, которые смогут проанализировать текст на наличие заданной эмоции и выявлять её, и проанализируем их качество.\n\n### Загрузка и обработка данных\nВ целом, данные довольно хорошие и не требуют большой обработки, но кое-что сделать все же будет необходимо.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/emotion-analysis-based-on-text/emotion_sentimen_dataset.csv', index_col = 0)\ndf = df.rename(columns={'Emotion': 'emotions'})\nprint(df.shape)\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T19:39:53.325891Z","iopub.execute_input":"2024-04-25T19:39:53.326877Z","iopub.status.idle":"2024-04-25T19:39:55.579502Z","shell.execute_reply.started":"2024-04-25T19:39:53.326835Z","shell.execute_reply":"2024-04-25T19:39:55.578328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.emotions.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:39:55.581382Z","iopub.execute_input":"2024-04-25T19:39:55.581736Z","iopub.status.idle":"2024-04-25T19:39:55.731554Z","shell.execute_reply.started":"2024-04-25T19:39:55.581706Z","shell.execute_reply":"2024-04-25T19:39:55.730293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как мы видим, есть довольно сильный дисбаланс классов. К сожалению, особо с этим ничего не поделаешь (не хотелось бы делать даунсэмплинг).\nТем не менее, для скорости обучения и простоты уменьшим количество нейтральных эмоций до 40000 и уберем последнюю эмоций с очень маленьким количеством записей.\n","metadata":{}},{"cell_type":"code","source":"df[df.emotions == 'neutral'] = df[df.emotions == 'neutral'].sample(40000) # берем 40к случайных записей\ndf = df[df.emotions != 'boredom'] # удаляем данную эмоцию\ndf = df.dropna()\ndf.emotions.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:39:55.733196Z","iopub.execute_input":"2024-04-25T19:39:55.733633Z","iopub.status.idle":"2024-04-25T19:39:56.649132Z","shell.execute_reply.started":"2024-04-25T19:39:55.733598Z","shell.execute_reply":"2024-04-25T19:39:56.648055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сделаем небольшую выборку, на которой можно более подробно посмотреть текст для обучения.","metadata":{}},{"cell_type":"code","source":"df_test = df.copy().sample(30, random_state = 73) # берем 30 случайных записей\nfor i in df_test.text:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:39:56.65123Z","iopub.execute_input":"2024-04-25T19:39:56.651591Z","iopub.status.idle":"2024-04-25T19:39:56.673517Z","shell.execute_reply.started":"2024-04-25T19:39:56.65156Z","shell.execute_reply":"2024-04-25T19:39:56.672375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее проведем обработку текста - приведем его к нижнему регистру,удалим специальные символы и цифры, а затем выделим токены и леммы, используя SpaCy.","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport string\n\nnlp = spacy.load(\"en_core_web_sm\") # загружаем модуль для английского языка\n\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:39:56.67502Z","iopub.execute_input":"2024-04-25T19:39:56.675388Z","iopub.status.idle":"2024-04-25T19:40:02.455458Z","shell.execute_reply.started":"2024-04-25T19:39:56.675354Z","shell.execute_reply":"2024-04-25T19:40:02.454368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Приведение текста к нижнему регистру\n    text = text.lower()\n    # Удаление лишних символов \n    text = ''.join([char for char in text if char not in string.punctuation and char not in '’—‘”“' and char.isalpha() or char == ' '])\n    # Удаление чисел\n    text = ''.join([char for char in text if not char.isdigit()])\n    # Инициализация объекта для обработки текста\n    doc = nlp(text)\n    # Токенизация текста и удаление стоп-слов\n    tokens = [token.text for token in doc if token.text not in STOP_WORDS]\n    # Лемматизация токенов\n    lemmatized_tokens = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS]\n    # Объединение обработанных слов\n    lemmatized_tokens = ' '.join(lemmatized_tokens)\n    # Возвращение предобработанного текста\n    return lemmatized_tokens\n\n# Примененение функции к тестовому датафрейму\ndf_test['preprocessed_text'] = df_test['text'].apply(preprocess_text) \ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:40:02.457672Z","iopub.execute_input":"2024-04-25T19:40:02.458385Z","iopub.status.idle":"2024-04-25T19:40:02.825777Z","shell.execute_reply.started":"2024-04-25T19:40:02.458343Z","shell.execute_reply":"2024-04-25T19:40:02.824686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"После этого применяем функцию ко всему датафрейму (займет много времени, потому его сохраняем)","metadata":{}},{"cell_type":"code","source":"# Примененение функции к основному датафрейму\ndf['preprocessed_text'] = df['text'].apply(preprocess_text) \ndf.head()\n\n# Сохранение нового датафрейма\ndf.to_csv('processed_text.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:40:07.444741Z","iopub.execute_input":"2024-04-25T19:40:07.445198Z","iopub.status.idle":"2024-04-25T20:13:44.890038Z","shell.execute_reply.started":"2024-04-25T19:40:07.445161Z","shell.execute_reply":"2024-04-25T20:13:44.888747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:28:54.919103Z","iopub.execute_input":"2024-04-25T20:28:54.919551Z","iopub.status.idle":"2024-04-25T20:28:54.931646Z","shell.execute_reply.started":"2024-04-25T20:28:54.919514Z","shell.execute_reply":"2024-04-25T20:28:54.930511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['encoded_emotions'] = label_encoder.fit_transform(df.emotions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Модели\n\n### Логистическая регрессия\nДля начала используем самую простую модель - логистическую регрессию.","metadata":{}},{"cell_type":"markdown","source":"Делим датафрейм на тренировочные данные и признаки.","metadata":{}},{"cell_type":"code","source":"X = df.preprocessed_text\ny = df.emotions","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:25:37.099703Z","iopub.execute_input":"2024-04-25T20:25:37.100673Z","iopub.status.idle":"2024-04-25T20:25:37.105552Z","shell.execute_reply.started":"2024-04-25T20:25:37.10063Z","shell.execute_reply":"2024-04-25T20:25:37.104438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Делаем векторизацию текста.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf = TfidfVectorizer(min_df=0, max_df=0.95, binary=False, ngram_range=(1,3))\ntf_train = tf.fit_transform(X)\ntf_test = tf.fit_transform(y)\nprint('Bow_tf_train',tf_train.shape)\nprint('Bow_tf_test',tf_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:26:56.392418Z","iopub.execute_input":"2024-04-25T20:26:56.392856Z","iopub.status.idle":"2024-04-25T20:27:11.372432Z","shell.execute_reply.started":"2024-04-25T20:26:56.392821Z","shell.execute_reply":"2024-04-25T20:27:11.371237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создаеем саму модель","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Создаем объект CountVectorizer\nvectorizer = CountVectorizer()\n\n# Конвертируем текст\nX_bow = vectorizer.fit_transform(df.text)\n\n# Делим данные на тренировочные и тестовые\nX_train, X_test, y_train, y_test = train_test_split(X_bow, df.emotions, test_size=0.2, random_state=73)\n\n# Инициализируем модель логистической регрессии\nlogistic = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=73)\n\n# Подставляем данные\nlr_bow = logistic.fit(X_train, y_train)\n\n# Делаем предсказания на тестовых данных\ny_pred = lr_bow.predict(X_test)\n\n# Рассчитываем точность\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Выдаем Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:29:17.649475Z","iopub.execute_input":"2024-04-25T20:29:17.65034Z","iopub.status.idle":"2024-04-25T20:30:52.401268Z","shell.execute_reply.started":"2024-04-25T20:29:17.650266Z","shell.execute_reply":"2024-04-25T20:30:52.400024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = {\n    'anger': \"I can't believe she said that to me! How dare she!\",\n    'empty': \"I feel like there's nothing left inside me. I'm just numb.\",\n    'enthusiasm': \"I can't wait to start my new project! I'm so excited about it!\",\n    'fun': \"Last night's party was so much fun! We danced all night long!\",\n    'happiness': \"I just got promoted at work! I'm really happy!\",\n    'hate': \"I can't stand that guy. Every time he speaks, I just feel hatred.\",\n    'love': \"Being with you makes me feel so loved and cherished.\",\n    'neutral': \"I don't really have strong feelings about this. It's just okay.\",\n    'relief': \"I finally finished my exams! What a relief!\",\n    'sadness': \"Losing my pet was one of the saddest moments of my life.\",\n    'surprise': \"I can't believe you remembered my birthday! What a pleasant surprise!\",\n    'sorry': \"I can't stop thinking about the future. What if things don't work out?\"}\n\n# Обработка текста \nprocessed_examples = {emotion: preprocess_text(text) for emotion, text in examples.items()} \n# Векторизация текста\nexample_features = vectorizer.transform(processed_examples.values())\n# Предсказание эмоций для каждого примера\npredicted_emotions = lr_bow.predict(example_features)\n# Подсчет правильных предсказаний\ncorrect_predictions = sum(1 for predicted_emotion, true_emotion in zip(predicted_emotions, examples.keys()) if predicted_emotion == true_emotion)\n\nprint(f\"Количество правильных предсказаний: {correct_predictions}/{len(examples)}\\n\")\n\nfor i, (emotion, text) in enumerate(processed_examples.items()):\n    print(f\"Текст: {text}, предсказанная эмоция: {predicted_emotions[i]} ({emotion})\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:49:43.067327Z","iopub.execute_input":"2024-04-25T20:49:43.067738Z","iopub.status.idle":"2024-04-25T20:49:43.179068Z","shell.execute_reply.started":"2024-04-25T20:49:43.067706Z","shell.execute_reply":"2024-04-25T20:49:43.17791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:52:10.518468Z","iopub.execute_input":"2024-04-25T20:52:10.519369Z","iopub.status.idle":"2024-04-25T20:52:11.229146Z","shell.execute_reply.started":"2024-04-25T20:52:10.519319Z","shell.execute_reply":"2024-04-25T20:52:11.227981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_bow, df.emotions, test_size=0.2, random_state=73)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:55:14.191729Z","iopub.execute_input":"2024-04-25T20:55:14.192176Z","iopub.status.idle":"2024-04-25T20:55:14.241675Z","shell.execute_reply.started":"2024-04-25T20:55:14.19214Z","shell.execute_reply":"2024-04-25T20:55:14.240508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Создание и обучение модели \nmodel = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, loss_function='MultiClass', random_seed=73)\nmodel.fit(X_train, y_train, verbose = 100)\n\ny_pred = model.predict(X_test)\n\n# Оценка производительности модели\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T20:58:38.12252Z","iopub.execute_input":"2024-04-25T20:58:38.123256Z","iopub.status.idle":"2024-04-25T22:00:06.484691Z","shell.execute_reply.started":"2024-04-25T20:58:38.123212Z","shell.execute_reply":"2024-04-25T22:00:06.483527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = {\n    'anger': \"I can't believe she said that to me! How dare she!\",\n    'empty': \"I feel like there's nothing left inside me. I'm just numb.\",\n    'enthusiasm': \"I can't wait to start my new project! I'm so excited about it!\",\n    'fun': \"Last night's party was so much fun! We danced all night long!\",\n    'happiness': \"I just got promoted at work! I'm really happy!\",\n    'hate': \"I can't stand that guy. Every time he speaks, I just feel hatred.\",\n    'love': \"Being with you makes me feel so loved and cherished.\",\n    'neutral': \"I don't really have strong feelings about this. It's just okay.\",\n    'relief': \"I finally finished my exams! What a relief!\",\n    'sadness': \"Losing my pet was one of the saddest moments of my life.\",\n    'surprise': \"I can't believe you remembered my birthday! What a pleasant surprise!\",\n    'sorry': \"I can't stop thinking about the future. What if things don't work out?\"}\n\n# Обработка текста \nprocessed_examples = {emotion: preprocess_text(text) for emotion, text in examples.items()} \n# Векторизация текста\nexample_features = vectorizer.transform(processed_examples.values())\n# Предсказание эмоций для каждого примера\npredicted_emotions = model.predict(example_features)\n\nprint(f\"Количество правильных предсказаний: {correct_predictions}/{len(examples)}\\n\")\n\nfor i, (emotion, text) in enumerate(processed_examples.items()):\n    print(f\"Текст: {text}, предсказанная эмоция: {predicted_emotions[i]} ({emotion})\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:11:22.938521Z","iopub.execute_input":"2024-04-25T22:11:22.939821Z","iopub.status.idle":"2024-04-25T22:11:23.096229Z","shell.execute_reply.started":"2024-04-25T22:11:22.939776Z","shell.execute_reply":"2024-04-25T22:11:23.095072Z"},"trusted":true},"execution_count":null,"outputs":[]}]}